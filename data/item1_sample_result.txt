SRLGRN: Semantic Role Labeling Graph Reasoning Network
This work deals with the challenge of learning and reasoning over multi-hop question answering (QA).	1	1
We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly.	2+3	2+3
The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges.	3	3
Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths.	3	3
Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.	4	4
-----------
On the Weak Link between Importance and Prunability of Attention Heads
Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency.	1	1
Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads.	2	2
We evaluate this on Transformer and BERT models on multiple NLP tasks.	3	3
Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy.	4	4
Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head.	4	4
On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance.	4	4
However, strategies that avoid pruning middle layers and consecutive layers perform better.	4	4
Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads.	4	4
Our results thus suggest that interpretation of attention heads does not strongly inform pruning.	4	4
-----------
A Generative Retrieval Model for Structured Documents
Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past.	1	1
However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead.	1	1
Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet.	1	1
In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task.	2+3	1+2
Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.	4	4
-----------
Acrostic Poem Generation
We propose a new task in the area of computational creativity: acrostic poem generation in English.	2	2
Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase.	1	1
We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme.	3	3
We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model.	4	4
Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems.	3	3
Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints.	4	4
Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.	3	3
-----------
A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation
Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.	1	1
However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.	1	1
To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.	2+3	2+3
Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).	3	3
We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.	3	3
Finally, these representations provide an attention-based context vector for the decoder.	3	3
We evaluate our proposed encoder on the Multi30K datasets.	3	3
Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.	4	4
-----------
EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets
Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks.	1	1
However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning.	1	1
Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process.	1	1
Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands.	1	1
In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models.	2	1+2
By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training.	3	3
We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks.	3	3
Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time.	4	4
Code is available at https://github.com/VITA-Group/EarlyBERT.	6	6